{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q chromadb langchain langchain_openai langchain_community python-dotenv requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenRouter model: openai/gpt-3.5-turbo\n",
      "OpenRouter API key loaded: Yes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenRouter API key from environment variables\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openrouter_model = os.getenv(\"OPENROUTER_MODEL\", \"openai/gpt-3.5-turbo\")\n",
    "\n",
    "print(f\"Using OpenRouter model: {openrouter_model}\")\n",
    "print(f\"OpenRouter API key loaded: {'Yes' if openrouter_api_key else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the latest LangChain packages\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.language_models import LLM\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import requests\n",
    "from typing import Any, List, Mapping, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom OpenRouter LLM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenRouterLLM(LLM):\n",
    "    \"\"\"Custom LLM class for OpenRouter API\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"openrouter\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {openrouter_api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"https://localhost\",  # Required by OpenRouter\n",
    "            \"X-Title\": \"ChromaDB Demo\"  # Optional, but good practice\n",
    "        }\n",
    "        data = {\n",
    "            \"model\": openrouter_model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": 2048,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response_json = response.json()\n",
    "        return response_json['choices'][0]['message']['content']\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model\": openrouter_model}\n",
    "\n",
    "# Create an instance of the OpenRouter LLM\n",
    "llm = OpenRouterLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: d:\\LLM-Ops\\Tutorials\\Vector-Databases\\data\n",
      "Sample file already exists: d:\\LLM-Ops\\Tutorials\\Vector-Databases\\data\\sample.txt\n",
      "AI models file already exists: d:\\LLM-Ops\\Tutorials\\Vector-Databases\\data\\ai_models.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a data directory if it doesn't exist\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"Created directory: {data_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {data_dir}\")\n",
    "\n",
    "# Create a sample text file for testing\n",
    "sample_file_path = os.path.join(data_dir, \"sample.txt\")\n",
    "if not os.path.exists(sample_file_path):\n",
    "    with open(sample_file_path, \"w\") as f:\n",
    "        f.write(\"This is a sample document for testing ChromaDB with OpenRouter.\\n\")\n",
    "        f.write(\"ChromaDB is a vector database that can be used for semantic search.\\n\")\n",
    "        f.write(\"OpenRouter provides access to various LLM models through a unified API.\\n\")\n",
    "    print(f\"Created sample file: {sample_file_path}\")\n",
    "else:\n",
    "    print(f\"Sample file already exists: {sample_file_path}\")\n",
    "\n",
    "# Create another sample file\n",
    "ai_file_path = os.path.join(data_dir, \"ai_models.txt\")\n",
    "if not os.path.exists(ai_file_path):\n",
    "    with open(ai_file_path, \"w\") as f:\n",
    "        f.write(\"Large Language Models (LLMs) are transforming how we interact with AI.\\n\")\n",
    "        f.write(\"Models like GPT-3.5, GPT-4, Claude, and Llama are available through OpenRouter.\\n\")\n",
    "        f.write(\"Vector databases help store and retrieve embeddings for semantic search applications.\\n\")\n",
    "    print(f\"Created AI models file: {ai_file_path}\")\n",
    "else:\n",
    "    print(f\"AI models file already exists: {ai_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents:\n",
      "\n",
      "Document: d:\\LLM-Ops\\Tutorials\\Vector-Databases\\data\\ai_models.txt\n",
      "Content: Large Language Models (LLMs) are transforming how we interact with AI.\n",
      "Models like GPT-3.5, GPT-4, C...\n",
      "\n",
      "Document: d:\\LLM-Ops\\Tutorials\\Vector-Databases\\data\\sample.txt\n",
      "Content: This is a sample document for testing ChromaDB with OpenRouter.\n",
      "ChromaDB is a vector database that c...\n"
     ]
    }
   ],
   "source": [
    "# Set up the document loader to use the local data directory\n",
    "loader = DirectoryLoader(data_dir, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Display the loaded documents\n",
    "print(f\"Loaded {len(documents)} documents:\")\n",
    "for doc in documents:\n",
    "    print(f\"\\nDocument: {doc.metadata['source']}\")\n",
    "    print(f\"Content: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings with OpenRouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error creating embeddings: Error code: 404 - {'error': {'message': 'Not Found', 'code': 404}}\n",
      "\n",
      "Troubleshooting tips:\n",
      "1. Check if your OpenRouter API key is valid\n",
      "2. Verify that OpenRouter supports the embedding model you're trying to use\n",
      "3. Check your internet connection\n",
      "4. Try using a different embedding model\n",
      "\n",
      "Falling back to HuggingFace embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_3336\\2746843392.py:29: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of OpenAI embeddings using OpenRouter\n",
    "try:\n",
    "    # First, test if the embeddings work by embedding a simple string\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "        openai_api_key=openrouter_api_key,\n",
    "        model=\"text-embedding-ada-002\",  # Note: removed \"openai/\" prefix which might cause issues\n",
    "        headers={\n",
    "            \"HTTP-Referer\": \"https://localhost\",  # Required by OpenRouter\n",
    "            \"X-Title\": \"ChromaDB Demo\"  # Optional, but good practice\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Test the embeddings with a simple string\n",
    "    test_embedding = embeddings.embed_query(\"This is a test\")\n",
    "    print(f\"✅ Embeddings test successful! Vector length: {len(test_embedding)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating embeddings: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Check if your OpenRouter API key is valid\")\n",
    "    print(\"2. Verify that OpenRouter supports the embedding model you're trying to use\")\n",
    "    print(\"3. Check your internet connection\")\n",
    "    print(\"4. Try using a different embedding model\")\n",
    "    \n",
    "    # Fallback to using HuggingFace embeddings if OpenRouter fails\n",
    "    print(\"\\nFalling back to HuggingFace embeddings...\")\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ChromaDB Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a Chroma vector store\n",
    "try:\n",
    "    vector_store = Chroma.from_documents(documents, embeddings)\n",
    "    print(\"ChromaDB vector store created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating vector store: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Check if your embeddings are working correctly\")\n",
    "    print(\"2. Verify that your documents are properly loaded\")\n",
    "    print(\"3. Try using a different embedding model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is ChromaDB used for?\n",
      "\n",
      "Results:\n",
      "\n",
      "Result 1:\n",
      "Source: d:\\LLM-Ops\\Tutorials\\Vector-Databases\\data\\sample.txt\n",
      "Content: This is a sample document for testing ChromaDB with OpenRouter.\n",
      "ChromaDB is a vector database that can be used for semantic search.\n",
      "OpenRouter provides access to various LLM models through a unified API.\n",
      "\n",
      "\n",
      "Result 2:\n",
      "Source: d:\\LLM-Ops\\Tutorials\\Vector-Databases\\data\\ai_models.txt\n",
      "Content: Large Language Models (LLMs) are transforming how we interact with AI.\n",
      "Models like GPT-3.5, GPT-4, Claude, and Llama are available through OpenRouter.\n",
      "Vector databases help store and retrieve embeddings for semantic search applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search\n",
    "try:\n",
    "    query = \"What is ChromaDB used for?\"\n",
    "    results = vector_store.similarity_search(query, k=2)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"\\nResults:\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"Source: {doc.metadata['source']}\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error performing similarity search: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Question-Answering Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Please respond to the user request only based on the given context\"),\n",
    "    (\"user\", \"Question: {question}\\nContext: {context}\")\n",
    "])\n",
    "\n",
    "# Create the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is OpenRouter and how does it relate to ChromaDB?\n",
      "\n",
      "Answer: OpenRouter provides access to various Large Language Models (LLMs) like GPT-3.5, GPT-4, Claude, and Llama through a unified API. ChromaDB is a vector database that can be used for semantic search applications.\n"
     ]
    }
   ],
   "source": [
    "# Define a question\n",
    "try:\n",
    "    question = \"What is OpenRouter and how does it relate to ChromaDB?\"\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    docs = vector_store.similarity_search(question, k=2)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Run the chain\n",
    "    response = chain.invoke({\"question\": question, \"context\": context})\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"\\nAnswer: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running QA chain: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Another Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What LLM models are available through OpenRouter?\n",
      "\n",
      "Answer: The LLM models available through OpenRouter include GPT-3.5, GPT-4, Claude, and Llama.\n"
     ]
    }
   ],
   "source": [
    "# Define another question\n",
    "try:\n",
    "    question = \"What LLM models are available through OpenRouter?\"\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    docs = vector_store.similarity_search(question, k=2)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Run the chain\n",
    "    response = chain.invoke({\"question\": question, \"context\": context})\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"\\nAnswer: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running QA chain: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_ops_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
